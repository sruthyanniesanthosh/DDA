{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b1207f",
   "metadata": {},
   "source": [
    "#  Apache Spark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163d7ac",
   "metadata": {},
   "source": [
    "## Part a) Basic Operations on Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d026d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/03 19:18:05 WARN Utils: Your hostname, Sruthys-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.103 instead (on interface en0)\n",
      "22/07/03 19:18:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/03 19:18:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/03 19:18:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import datediff,col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark_dist_explore import hist\n",
    "\n",
    "#Initialize sparkcontext\n",
    "sc = SparkContext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6a16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize 2 lists of words\n",
    "\n",
    "a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcc19ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Creating rdd objects for the lists\n",
    "rdd_a=sc.parallelize(a)\n",
    "rdd_a.collect()\n",
    "\n",
    "#adding a label with each word in list\n",
    "rdda = rdd_a.map(lambda x: (x,'a'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54ccaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating rdd object \n",
    "rdd_b = sc.parallelize(b)\n",
    "rddb = rdd_b.map(lambda x: (x,'b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830f5a8",
   "metadata": {},
   "source": [
    "Performing rightOuterJoin and fullOuterJoin operations between a and b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3606c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('apache', (None, 'b')),\n",
       " ('class', ('a', None)),\n",
       " ('context', ('a', None)),\n",
       " ('create', ('a', None)),\n",
       " ('lambda', (None, 'b')),\n",
       " ('operation', (None, 'b')),\n",
       " ('parallel', (None, 'b')),\n",
       " ('partition', (None, 'b')),\n",
       " ('python', ('a', None)),\n",
       " ('rdd', ('a', None)),\n",
       " ('scala', (None, 'b')),\n",
       " ('spark', ('a', None))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdda.fullOuterJoin(rddb).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b4acb",
   "metadata": {},
   "source": [
    "We see that fullouterjoin returns a list having each word in both the lists along with whether they occur in the corresponding the lists. If they occur the label we had given earlier is present, else None. We see that none of the words occur in both the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b5f95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('class', (None, 'a')),\n",
       " ('context', (None, 'a')),\n",
       " ('create', (None, 'a')),\n",
       " ('python', (None, 'a')),\n",
       " ('rdd', (None, 'a')),\n",
       " ('spark', (None, 'a'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rddb.rightOuterJoin(rdda).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbbddf",
   "metadata": {},
   "source": [
    "In rightouterjoin, it returns a list having the words in the rdd_a only. It shows that none of those words occur in the other rdd as it returns none. And they all are member of list a as it returns that label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd9992d",
   "metadata": {},
   "source": [
    "Using map and reduce functions to count how many times the character \"s\" appears in all a and\n",
    "b.\n",
    "\n",
    "We define a function that counts the occurrence of letter s in each word in the list. And that function is passed with the map function of each rdd. Then we combine the outputs of both rdd maps and perform reduce operation to count the total number s characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71d89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Func(x):\n",
    "    return (x.count('s'))\n",
    "\n",
    "count_a = rdd_a.map(Func)\n",
    "count_b = rdd_b.map(Func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ddc4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The occurence of s in each word in rdd_a . Total occurence - 3\n",
    "count_a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39de7497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The occurence of s in each word in rdd_a . Total occurence - 1\n",
    "count_b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee8c0a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Union function combines both the count lists\n",
    "reduce_rdd = count_a.union(count_b)\n",
    "\n",
    "#we reduce function yields the sum of input characters\n",
    "reduce_rdd.reduce(lambda x, y : x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b6577",
   "metadata": {},
   "source": [
    "Using aggregate function to count how many times the character \"s\" appears in all a and b.\n",
    "\n",
    "We first combine the rdd's and then map the count in each word and create data frame with the words and count of 's' in each word as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f37d52de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'rdd',\n",
       " 'python',\n",
       " 'context',\n",
       " 'create',\n",
       " 'class',\n",
       " 'operation',\n",
       " 'apache',\n",
       " 'scala',\n",
       " 'lambda',\n",
       " 'parallel',\n",
       " 'partition']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combines the lists\n",
    "full = (rdd_a.union(rdd_b))\n",
    "full.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd28ec2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1),\n",
       " ('rdd', 0),\n",
       " ('python', 0),\n",
       " ('context', 0),\n",
       " ('create', 0),\n",
       " ('class', 2),\n",
       " ('operation', 0),\n",
       " ('apache', 0),\n",
       " ('scala', 1),\n",
       " ('lambda', 0),\n",
       " ('parallel', 0),\n",
       " ('partition', 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function which gives each word and the number of 's' characters in each word\n",
    "def Func_new(x):\n",
    "    return (x,x.count('s'))\n",
    "\n",
    "#map function\n",
    "count_full = full.map(Func_new)\n",
    "count_full.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba58410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(word='spark', count=1),\n",
       " Row(word='rdd', count=0),\n",
       " Row(word='python', count=0),\n",
       " Row(word='context', count=0),\n",
       " Row(word='create', count=0),\n",
       " Row(word='class', count=2),\n",
       " Row(word='operation', count=0),\n",
       " Row(word='apache', count=0),\n",
       " Row(word='scala', count=1),\n",
       " Row(word='lambda', count=0),\n",
       " Row(word='parallel', count=0),\n",
       " Row(word='partition', count=0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# creating dataframe from the output of map function\n",
    "\n",
    "spark = SparkSession.builder.appName(\"groupbyagg\").getOrCreate()\n",
    "df = spark.createDataFrame(data=count_full, schema=[\"word\",\"count\"])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b4e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===>                                                     (1 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Finding the count of s characters in all the lists using aggregate on the count column\n",
    "df.agg({'count':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8509b2b",
   "metadata": {},
   "source": [
    "### Part b) Basic Operations on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db2bfd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- points: long (nullable = true)\n",
      " |-- s_id: long (nullable = true)\n",
      "\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|  null|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating data frame from json file\n",
    "df_students = spark.read.json(\"Downloads/students.json\")\n",
    "df_students.printSchema()\n",
    "df_students.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017f9cb",
   "metadata": {},
   "source": [
    "1. Replace the null value(s) in column points by the mean of all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49a1023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|  null|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#First fill null with 0\n",
    "df_students.na.fill(0,\"points\")\n",
    "df_students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e0d2df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.736842105263158"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the mean of the points column\n",
    "df_mean = df_students.agg({'points':'avg'}).collect()\n",
    "df_mean[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bb4bd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling the null value with mean\n",
    "df_students = df_students.fillna(value = df_mean[0][0], subset=['points'] )\n",
    "df_students.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40006d8",
   "metadata": {},
   "source": [
    "2. Replace the null value(s) in column dob and column last name by \"unknown\" and \"--\" respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0b47b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_students = df_students.fillna(value =\"unknown\", subset=['dob'])\n",
    "df_students = df_students.fillna(value =\"--\", subset=['last_name'])\n",
    "df_students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff2ab01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[course: string, dob: string, first_name: string, last_name: string, points: bigint, s_id: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fdebd",
   "metadata": {},
   "source": [
    "3. Convert all dates to dd-mm-yyyy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4fdbc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|               dob|      date|\n",
      "+------------------+----------+\n",
      "|  October 14, 1983|1983-10-14|\n",
      "|September 26, 1980|1980-09-26|\n",
      "|     June 12, 1982|1982-06-12|\n",
      "|     April 5, 1987|1987-04-05|\n",
      "|  November 1, 1978|1978-11-01|\n",
      "|  17 February 1981|      null|\n",
      "|    1 January 1984|      null|\n",
      "|  January 13, 1978|1978-01-13|\n",
      "|  26 December 1989|      null|\n",
      "|  30 December 1987|      null|\n",
      "|     June 12, 1975|1975-06-12|\n",
      "|      July 2, 1985|1985-07-02|\n",
      "|     July 22, 1980|1980-07-22|\n",
      "|   7 February 1986|      null|\n",
      "|      May 18, 1987|1987-05-18|\n",
      "|   August 10, 1984|1984-08-10|\n",
      "|  16 December 1990|      null|\n",
      "|           unknown|      null|\n",
      "|      7 March 1980|      null|\n",
      "|      June 2, 1985|1985-06-02|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting all date of MMMM d,yyyy to date format \n",
    "df_students1 = df_students.select( col(\"dob\"),to_date(col(\"dob\"),\"MMMM d, yyyy\").alias(\"date\"))\n",
    "df_students1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bff05b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|               dob|  date_new|\n",
      "+------------------+----------+\n",
      "|  October 14, 1983|      null|\n",
      "|September 26, 1980|      null|\n",
      "|     June 12, 1982|      null|\n",
      "|     April 5, 1987|      null|\n",
      "|  November 1, 1978|      null|\n",
      "|  17 February 1981|1981-02-17|\n",
      "|    1 January 1984|1984-01-01|\n",
      "|  January 13, 1978|      null|\n",
      "|  26 December 1989|1989-12-26|\n",
      "|  30 December 1987|1987-12-30|\n",
      "|     June 12, 1975|      null|\n",
      "|      July 2, 1985|      null|\n",
      "|     July 22, 1980|      null|\n",
      "|   7 February 1986|1986-02-07|\n",
      "|      May 18, 1987|      null|\n",
      "|   August 10, 1984|      null|\n",
      "|  16 December 1990|1990-12-16|\n",
      "|           unknown|      null|\n",
      "|      7 March 1980|1980-03-07|\n",
      "|      June 2, 1985|      null|\n",
      "+------------------+----------+\n",
      "\n",
      "+------------------+----------+----------+\n",
      "|               dob|      date|  date_new|\n",
      "+------------------+----------+----------+\n",
      "|  October 14, 1983|1983-10-14|      null|\n",
      "|September 26, 1980|1980-09-26|      null|\n",
      "|     June 12, 1982|1982-06-12|      null|\n",
      "|     April 5, 1987|1987-04-05|      null|\n",
      "|  November 1, 1978|1978-11-01|      null|\n",
      "|  17 February 1981|      null|1981-02-17|\n",
      "|    1 January 1984|      null|1984-01-01|\n",
      "|  January 13, 1978|1978-01-13|      null|\n",
      "|  26 December 1989|      null|1989-12-26|\n",
      "|  30 December 1987|      null|1987-12-30|\n",
      "|     June 12, 1975|1975-06-12|      null|\n",
      "|      July 2, 1985|1985-07-02|      null|\n",
      "|     July 22, 1980|1980-07-22|      null|\n",
      "|   7 February 1986|      null|1986-02-07|\n",
      "|      May 18, 1987|1987-05-18|      null|\n",
      "|   August 10, 1984|1984-08-10|      null|\n",
      "|  16 December 1990|      null|1990-12-16|\n",
      "|           unknown|      null|      null|\n",
      "|      7 March 1980|      null|1980-03-07|\n",
      "|      June 2, 1985|1985-06-02|      null|\n",
      "+------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting the dates of format d MMMM yyyy to date format\n",
    "df_students2 = df_students.select(col(\"dob\"),to_date(col(\"dob\"),\"d MMMM yyyy\").alias(\"date_new\")) \n",
    "df_students2.show()\n",
    "\n",
    "#Now I combine both the dates got to geta dataframe having original dobs, and the converted ones\n",
    "combine = df_students1.join(df_students2, on=[\"dob\"])\n",
    "combine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e26e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the converted dobs to one column such that they replace the null values \n",
    "combine = combine.withColumn(\"dob_new\",f.coalesce(\"date\",\"date_new\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c67b42e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "|               dob|            course|first_name|last_name|points|s_id|   dob_new|\n",
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "|  October 14, 1983|Humanities and Art|      Alan|      Joe|    10|   1|1983-10-14|\n",
      "|September 26, 1980|  Computer Science|    Martin|  Genberg|    17|   2|1980-09-26|\n",
      "|     June 12, 1982|    Graphic Design|     Athur|   Watson|    16|   3|1982-06-12|\n",
      "|     April 5, 1987|    Graphic Design|  Anabelle|  Sanberg|    12|   4|1987-04-05|\n",
      "|  November 1, 1978|        Psychology|      Kira| Schommer|    11|   5|1978-11-01|\n",
      "|  17 February 1981|          Business| Christian|   Kiriam|    10|   6|1981-02-17|\n",
      "|    1 January 1984|  Machine Learning|   Barbara|  Ballard|    14|   7|1984-01-01|\n",
      "|  January 13, 1978|     Deep Learning|      John|       --|    10|   8|1978-01-13|\n",
      "|  26 December 1989|  Machine Learning|    Marcus|   Carson|    15|   9|1989-12-26|\n",
      "|  30 December 1987|           Physics|     Marta|   Brooks|    11|  10|1987-12-30|\n",
      "|     June 12, 1975|    Data Analytics|     Holly| Schwartz|    12|  11|1975-06-12|\n",
      "|      July 2, 1985|  Computer Science|     April|    Black|    11|  12|1985-07-02|\n",
      "|     July 22, 1980|  Computer Science|     Irene|  Bradley|    13|  13|1980-07-22|\n",
      "|   7 February 1986|        Psychology|      Mark|    Weber|    12|  14|1986-02-07|\n",
      "|      May 18, 1987|       Informatics|     Rosie|   Norman|     9|  15|1987-05-18|\n",
      "|   August 10, 1984|          Business|    Martin|   Steele|     7|  16|1984-08-10|\n",
      "|  16 December 1990|  Machine Learning|     Colin| Martinez|     9|  17|1990-12-16|\n",
      "|           unknown|    Data Analytics|   Bridget|    Twain|     6|  18|      null|\n",
      "|      7 March 1980|          Business|   Darlene|    Mills|    19|  19|1980-03-07|\n",
      "|      June 2, 1985|    Data Analytics|   Zachary|       --|    10|  20|1985-06-02|\n",
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping unwanted columns\n",
    "df_students = df_students.join(combine, on=[\"dob\"]).drop(\"date\",\"date_new\")\n",
    "#Replacing null values \n",
    "df_students = df_students.fillna(value =\"unknown\", subset=['dob_new'])\n",
    "\n",
    "df_students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cd4dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "|               dob|            course|first_name|last_name|points|s_id|   dob_new|\n",
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "|  October 14, 1983|Humanities and Art|      Alan|      Joe|    10|   1|1983-10-14|\n",
      "|September 26, 1980|  Computer Science|    Martin|  Genberg|    17|   2|1980-09-26|\n",
      "|     June 12, 1982|    Graphic Design|     Athur|   Watson|    16|   3|1982-06-12|\n",
      "|     April 5, 1987|    Graphic Design|  Anabelle|  Sanberg|    12|   4|1987-04-05|\n",
      "|  November 1, 1978|        Psychology|      Kira| Schommer|    11|   5|1978-11-01|\n",
      "|  17 February 1981|          Business| Christian|   Kiriam|    10|   6|1981-02-17|\n",
      "|    1 January 1984|  Machine Learning|   Barbara|  Ballard|    14|   7|1984-01-01|\n",
      "|  January 13, 1978|     Deep Learning|      John|       --|    10|   8|1978-01-13|\n",
      "|  26 December 1989|  Machine Learning|    Marcus|   Carson|    15|   9|1989-12-26|\n",
      "|  30 December 1987|           Physics|     Marta|   Brooks|    11|  10|1987-12-30|\n",
      "|     June 12, 1975|    Data Analytics|     Holly| Schwartz|    12|  11|1975-06-12|\n",
      "|      July 2, 1985|  Computer Science|     April|    Black|    11|  12|1985-07-02|\n",
      "|     July 22, 1980|  Computer Science|     Irene|  Bradley|    13|  13|1980-07-22|\n",
      "|   7 February 1986|        Psychology|      Mark|    Weber|    12|  14|1986-02-07|\n",
      "|      May 18, 1987|       Informatics|     Rosie|   Norman|     9|  15|1987-05-18|\n",
      "|   August 10, 1984|          Business|    Martin|   Steele|     7|  16|1984-08-10|\n",
      "|  16 December 1990|  Machine Learning|     Colin| Martinez|     9|  17|1990-12-16|\n",
      "|           unknown|    Data Analytics|   Bridget|    Twain|     6|  18|0000-00-00|\n",
      "|      7 March 1980|          Business|   Darlene|    Mills|    19|  19|1980-03-07|\n",
      "|      June 2, 1985|    Data Analytics|   Zachary|       --|    10|  20|1985-06-02|\n",
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For type date, fillna doesnt work, hence replacing it to 0000-00-00\n",
    "df_students = df_students.fillna(value =\"unknown\", subset=[\"dob_new\"])\n",
    "for t in df_students.dtypes:\n",
    "     if t[1] == 'date':\n",
    "            df_students = df_students.withColumn(t[0], f.coalesce(t[0], f.lit('0000-00-00')))\n",
    "   \n",
    "    \n",
    "\n",
    "df_students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec8e1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|   dob_new|date_format|\n",
      "+----------+-----------+\n",
      "|1983-10-14| 14-10-1983|\n",
      "|1980-09-26| 26-09-1980|\n",
      "|1982-06-12| 12-06-1982|\n",
      "|1987-04-05| 05-04-1987|\n",
      "|1978-11-01| 01-11-1978|\n",
      "|1981-02-17| 17-02-1981|\n",
      "|1984-01-01| 01-01-1984|\n",
      "|1978-01-13| 13-01-1978|\n",
      "|1989-12-26| 26-12-1989|\n",
      "|1987-12-30| 30-12-1987|\n",
      "|1975-06-12| 12-06-1975|\n",
      "|1985-07-02| 02-07-1985|\n",
      "|1980-07-22| 22-07-1980|\n",
      "|1986-02-07| 07-02-1986|\n",
      "|1987-05-18| 18-05-1987|\n",
      "|1984-08-10| 10-08-1984|\n",
      "|1990-12-16| 16-12-1990|\n",
      "|0000-00-00|    unknown|\n",
      "|1980-03-07| 07-03-1980|\n",
      "|1985-06-02| 02-06-1985|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Changing the format to the required dd-mm-yyy\n",
    "combine_n = df_students.select(*[col(\"dob_new\"), \n",
    "    date_format(col(\"dob_new\"), \"dd-MM-yyyy\").alias(\"date_format\")] )\n",
    "combine_n = combine_n.fillna(value =\"unknown\", subset=['dob_new','date_format'])\n",
    "combine_n.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4f4fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|   dob_new|date_format|\n",
      "+----------+-----------+\n",
      "|1983-10-14| 14-10-1983|\n",
      "|1980-09-26| 26-09-1980|\n",
      "|1982-06-12| 12-06-1982|\n",
      "|1987-04-05| 05-04-1987|\n",
      "|1978-11-01| 01-11-1978|\n",
      "|1981-02-17| 17-02-1981|\n",
      "|1984-01-01| 01-01-1984|\n",
      "|1978-01-13| 13-01-1978|\n",
      "|1989-12-26| 26-12-1989|\n",
      "|1987-12-30| 30-12-1987|\n",
      "|1975-06-12| 12-06-1975|\n",
      "|1985-07-02| 02-07-1985|\n",
      "|1980-07-22| 22-07-1980|\n",
      "|1986-02-07| 07-02-1986|\n",
      "|1987-05-18| 18-05-1987|\n",
      "|1984-08-10| 10-08-1984|\n",
      "|1990-12-16| 16-12-1990|\n",
      "|0000-00-00|    unknown|\n",
      "|1980-03-07| 07-03-1980|\n",
      "|1985-06-02| 02-06-1985|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+------------------+------------------+----------+---------+------+----+-----------+\n",
      "|   dob_new|               dob|            course|first_name|last_name|points|s_id|date_format|\n",
      "+----------+------------------+------------------+----------+---------+------+----+-----------+\n",
      "|0000-00-00|           unknown|    Data Analytics|   Bridget|    Twain|     6|  18|    unknown|\n",
      "|1975-06-12|     June 12, 1975|    Data Analytics|     Holly| Schwartz|    12|  11| 12-06-1975|\n",
      "|1978-01-13|  January 13, 1978|     Deep Learning|      John|       --|    10|   8| 13-01-1978|\n",
      "|1978-11-01|  November 1, 1978|        Psychology|      Kira| Schommer|    11|   5| 01-11-1978|\n",
      "|1980-03-07|      7 March 1980|          Business|   Darlene|    Mills|    19|  19| 07-03-1980|\n",
      "|1980-07-22|     July 22, 1980|  Computer Science|     Irene|  Bradley|    13|  13| 22-07-1980|\n",
      "|1980-09-26|September 26, 1980|  Computer Science|    Martin|  Genberg|    17|   2| 26-09-1980|\n",
      "|1981-02-17|  17 February 1981|          Business| Christian|   Kiriam|    10|   6| 17-02-1981|\n",
      "|1982-06-12|     June 12, 1982|    Graphic Design|     Athur|   Watson|    16|   3| 12-06-1982|\n",
      "|1983-10-14|  October 14, 1983|Humanities and Art|      Alan|      Joe|    10|   1| 14-10-1983|\n",
      "|1984-01-01|    1 January 1984|  Machine Learning|   Barbara|  Ballard|    14|   7| 01-01-1984|\n",
      "|1984-08-10|   August 10, 1984|          Business|    Martin|   Steele|     7|  16| 10-08-1984|\n",
      "|1985-06-02|      June 2, 1985|    Data Analytics|   Zachary|       --|    10|  20| 02-06-1985|\n",
      "|1985-07-02|      July 2, 1985|  Computer Science|     April|    Black|    11|  12| 02-07-1985|\n",
      "|1986-02-07|   7 February 1986|        Psychology|      Mark|    Weber|    12|  14| 07-02-1986|\n",
      "|1987-04-05|     April 5, 1987|    Graphic Design|  Anabelle|  Sanberg|    12|   4| 05-04-1987|\n",
      "|1987-05-18|      May 18, 1987|       Informatics|     Rosie|   Norman|     9|  15| 18-05-1987|\n",
      "|1987-12-30|  30 December 1987|           Physics|     Marta|   Brooks|    11|  10| 30-12-1987|\n",
      "|1989-12-26|  26 December 1989|  Machine Learning|    Marcus|   Carson|    15|   9| 26-12-1989|\n",
      "|1990-12-16|  16 December 1990|  Machine Learning|     Colin| Martinez|     9|  17| 16-12-1990|\n",
      "+----------+------------------+------------------+----------+---------+------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding the required date format column to original dataframe.\n",
    "combine_n = combine_n.fillna(value =\"unknown\", subset=['dob_new'])\n",
    "combine_n.show()\n",
    "\n",
    "df_students = df_students.join(combine_n, on=[\"dob_new\"])\n",
    "df_students.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c85384",
   "metadata": {},
   "source": [
    "4. Insert a new column age and calculate the current age of all students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "654e2a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+----------+---------+------+----+-----------+----------+\n",
      "|   dob_new|               dob|            course|first_name|last_name|points|s_id|date_format|   current|\n",
      "+----------+------------------+------------------+----------+---------+------+----+-----------+----------+\n",
      "|0000-00-00|           unknown|    Data Analytics|   Bridget|    Twain|     6|  18|    unknown|2022-07-03|\n",
      "|1975-06-12|     June 12, 1975|    Data Analytics|     Holly| Schwartz|    12|  11| 12-06-1975|2022-07-03|\n",
      "|1978-01-13|  January 13, 1978|     Deep Learning|      John|       --|    10|   8| 13-01-1978|2022-07-03|\n",
      "|1978-11-01|  November 1, 1978|        Psychology|      Kira| Schommer|    11|   5| 01-11-1978|2022-07-03|\n",
      "|1980-03-07|      7 March 1980|          Business|   Darlene|    Mills|    19|  19| 07-03-1980|2022-07-03|\n",
      "|1980-07-22|     July 22, 1980|  Computer Science|     Irene|  Bradley|    13|  13| 22-07-1980|2022-07-03|\n",
      "|1980-09-26|September 26, 1980|  Computer Science|    Martin|  Genberg|    17|   2| 26-09-1980|2022-07-03|\n",
      "|1981-02-17|  17 February 1981|          Business| Christian|   Kiriam|    10|   6| 17-02-1981|2022-07-03|\n",
      "|1982-06-12|     June 12, 1982|    Graphic Design|     Athur|   Watson|    16|   3| 12-06-1982|2022-07-03|\n",
      "|1983-10-14|  October 14, 1983|Humanities and Art|      Alan|      Joe|    10|   1| 14-10-1983|2022-07-03|\n",
      "|1984-01-01|    1 January 1984|  Machine Learning|   Barbara|  Ballard|    14|   7| 01-01-1984|2022-07-03|\n",
      "|1984-08-10|   August 10, 1984|          Business|    Martin|   Steele|     7|  16| 10-08-1984|2022-07-03|\n",
      "|1985-06-02|      June 2, 1985|    Data Analytics|   Zachary|       --|    10|  20| 02-06-1985|2022-07-03|\n",
      "|1985-07-02|      July 2, 1985|  Computer Science|     April|    Black|    11|  12| 02-07-1985|2022-07-03|\n",
      "|1986-02-07|   7 February 1986|        Psychology|      Mark|    Weber|    12|  14| 07-02-1986|2022-07-03|\n",
      "|1987-04-05|     April 5, 1987|    Graphic Design|  Anabelle|  Sanberg|    12|   4| 05-04-1987|2022-07-03|\n",
      "|1987-05-18|      May 18, 1987|       Informatics|     Rosie|   Norman|     9|  15| 18-05-1987|2022-07-03|\n",
      "|1987-12-30|  30 December 1987|           Physics|     Marta|   Brooks|    11|  10| 30-12-1987|2022-07-03|\n",
      "|1989-12-26|  26 December 1989|  Machine Learning|    Marcus|   Carson|    15|   9| 26-12-1989|2022-07-03|\n",
      "|1990-12-16|  16 December 1990|  Machine Learning|     Colin| Martinez|     9|  17| 16-12-1990|2022-07-03|\n",
      "+----------+------------------+------------------+----------+---------+------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Adding the current date in a column\n",
    "df2 = df_students.withColumn(\"current\", f.current_date())\n",
    "\n",
    "\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "528badf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding age by getting the difference in years between dob and current date\n",
    "\n",
    "df_f = df2.withColumn(\"age\", datediff(col(\"current\"),col(\"dob_new\"))/365.25)\n",
    "#converting to integer\n",
    "new_df = df_f.withColumn(\"age\", df_f[\"age\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18473200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "|               dob|            course|first_name|last_name|points|s_id|date_format| age|\n",
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "|           unknown|    Data Analytics|   Bridget|    Twain|     6|  18|    unknown|null|\n",
      "|     June 12, 1975|    Data Analytics|     Holly| Schwartz|    12|  11| 12-06-1975|  47|\n",
      "|  January 13, 1978|     Deep Learning|      John|       --|    10|   8| 13-01-1978|  44|\n",
      "|  November 1, 1978|        Psychology|      Kira| Schommer|    11|   5| 01-11-1978|  43|\n",
      "|      7 March 1980|          Business|   Darlene|    Mills|    19|  19| 07-03-1980|  42|\n",
      "|     July 22, 1980|  Computer Science|     Irene|  Bradley|    13|  13| 22-07-1980|  41|\n",
      "|September 26, 1980|  Computer Science|    Martin|  Genberg|    17|   2| 26-09-1980|  41|\n",
      "|  17 February 1981|          Business| Christian|   Kiriam|    10|   6| 17-02-1981|  41|\n",
      "|     June 12, 1982|    Graphic Design|     Athur|   Watson|    16|   3| 12-06-1982|  40|\n",
      "|  October 14, 1983|Humanities and Art|      Alan|      Joe|    10|   1| 14-10-1983|  38|\n",
      "|    1 January 1984|  Machine Learning|   Barbara|  Ballard|    14|   7| 01-01-1984|  38|\n",
      "|   August 10, 1984|          Business|    Martin|   Steele|     7|  16| 10-08-1984|  37|\n",
      "|      June 2, 1985|    Data Analytics|   Zachary|       --|    10|  20| 02-06-1985|  37|\n",
      "|      July 2, 1985|  Computer Science|     April|    Black|    11|  12| 02-07-1985|  37|\n",
      "|   7 February 1986|        Psychology|      Mark|    Weber|    12|  14| 07-02-1986|  36|\n",
      "|     April 5, 1987|    Graphic Design|  Anabelle|  Sanberg|    12|   4| 05-04-1987|  35|\n",
      "|      May 18, 1987|       Informatics|     Rosie|   Norman|     9|  15| 18-05-1987|  35|\n",
      "|  30 December 1987|           Physics|     Marta|   Brooks|    11|  10| 30-12-1987|  34|\n",
      "|  26 December 1989|  Machine Learning|    Marcus|   Carson|    15|   9| 26-12-1989|  32|\n",
      "|  16 December 1990|  Machine Learning|     Colin| Martinez|     9|  17| 16-12-1990|  31|\n",
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping unwanted columns\n",
    "df_new2 = new_df.drop(\"dob_new\",\"current\")\n",
    "df_new2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d6ef4",
   "metadata": {},
   "source": [
    "Letâ€™s consider granting some points for good performed students in the class. For each student,\n",
    "if his point is larger than 1 standard deviation of all points, then we update his current point to\n",
    "20, which is the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3108a5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   stddev(points)|\n",
      "+-----------------+\n",
      "|3.246050231475656|\n",
      "+-----------------+\n",
      "\n",
      "+-----------+\n",
      "|avg(points)|\n",
      "+-----------+\n",
      "|       11.7|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Finding mean and standard deviation of the column points\n",
    "std = df_new2.agg({'points': 'stddev' })\n",
    "mean = df_new2.agg({'points':'mean'})\n",
    "\n",
    "std.show()\n",
    "mean.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dfc1e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "|               dob|            course|first_name|last_name|points|s_id|date_format| age|\n",
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "|           unknown|    Data Analytics|   Bridget|    Twain|     6|  18|    unknown|null|\n",
      "|     June 12, 1975|    Data Analytics|     Holly| Schwartz|    12|  11| 12-06-1975|  47|\n",
      "|  January 13, 1978|     Deep Learning|      John|       --|    10|   8| 13-01-1978|  44|\n",
      "|  November 1, 1978|        Psychology|      Kira| Schommer|    11|   5| 01-11-1978|  43|\n",
      "|      7 March 1980|          Business|   Darlene|    Mills|    20|  19| 07-03-1980|  42|\n",
      "|     July 22, 1980|  Computer Science|     Irene|  Bradley|    13|  13| 22-07-1980|  41|\n",
      "|September 26, 1980|  Computer Science|    Martin|  Genberg|    20|   2| 26-09-1980|  41|\n",
      "|  17 February 1981|          Business| Christian|   Kiriam|    10|   6| 17-02-1981|  41|\n",
      "|     June 12, 1982|    Graphic Design|     Athur|   Watson|    20|   3| 12-06-1982|  40|\n",
      "|  October 14, 1983|Humanities and Art|      Alan|      Joe|    10|   1| 14-10-1983|  38|\n",
      "|    1 January 1984|  Machine Learning|   Barbara|  Ballard|    14|   7| 01-01-1984|  38|\n",
      "|   August 10, 1984|          Business|    Martin|   Steele|     7|  16| 10-08-1984|  37|\n",
      "|      June 2, 1985|    Data Analytics|   Zachary|       --|    10|  20| 02-06-1985|  37|\n",
      "|      July 2, 1985|  Computer Science|     April|    Black|    11|  12| 02-07-1985|  37|\n",
      "|   7 February 1986|        Psychology|      Mark|    Weber|    12|  14| 07-02-1986|  36|\n",
      "|     April 5, 1987|    Graphic Design|  Anabelle|  Sanberg|    12|   4| 05-04-1987|  35|\n",
      "|      May 18, 1987|       Informatics|     Rosie|   Norman|     9|  15| 18-05-1987|  35|\n",
      "|  30 December 1987|           Physics|     Marta|   Brooks|    11|  10| 30-12-1987|  34|\n",
      "|  26 December 1989|  Machine Learning|    Marcus|   Carson|    20|   9| 26-12-1989|  32|\n",
      "|  16 December 1990|  Machine Learning|     Colin| Martinez|     9|  17| 16-12-1990|  31|\n",
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The points having value greater than 1 std is asssigned as 20.\n",
    "df3 = df_new2.withColumn(\"points\", when(df_new2.points >= 15,20) \\\n",
    "      .otherwise(df_new2.points))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7ec8c",
   "metadata": {},
   "source": [
    "Create a histogram on the new points created in the task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db8f1f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 0., 6., 3., 3., 2., 0., 0., 0., 4.]),\n",
       " array([ 6. ,  7.4,  8.8, 10.2, 11.6, 13. , 14.4, 15.8, 17.2, 18.6, 20. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"points\")\n",
    "ax.set_ylabel(\"Number of students\")\n",
    "ax.set_title(\"Histogram of points of different students\")\n",
    "hist(ax, df3.select('points'), bins = 10, color=['red'])\n",
    "\n",
    "#Histogram shows that 6 students got around 10 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71986948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "|dob               |course            |first_name|last_name|points|s_id|date_format|age |\n",
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "|October 14, 1983  |Humanities and Art|Alan      |Joe      |10    |1   |14-10-1983 |38  |\n",
      "|September 26, 1980|Computer Science  |Martin    |Genberg  |20    |2   |26-09-1980 |41  |\n",
      "|June 12, 1982     |Graphic Design    |Athur     |Watson   |20    |3   |12-06-1982 |40  |\n",
      "|April 5, 1987     |Graphic Design    |Anabelle  |Sanberg  |12    |4   |05-04-1987 |35  |\n",
      "|November 1, 1978  |Psychology        |Kira      |Schommer |11    |5   |01-11-1978 |43  |\n",
      "|17 February 1981  |Business          |Christian |Kiriam   |10    |6   |17-02-1981 |41  |\n",
      "|1 January 1984    |Machine Learning  |Barbara   |Ballard  |14    |7   |01-01-1984 |38  |\n",
      "|January 13, 1978  |Deep Learning     |John      |--       |10    |8   |13-01-1978 |44  |\n",
      "|26 December 1989  |Machine Learning  |Marcus    |Carson   |20    |9   |26-12-1989 |32  |\n",
      "|30 December 1987  |Physics           |Marta     |Brooks   |11    |10  |30-12-1987 |34  |\n",
      "|June 12, 1975     |Data Analytics    |Holly     |Schwartz |12    |11  |12-06-1975 |47  |\n",
      "|July 2, 1985      |Computer Science  |April     |Black    |11    |12  |02-07-1985 |37  |\n",
      "|July 22, 1980     |Computer Science  |Irene     |Bradley  |13    |13  |22-07-1980 |41  |\n",
      "|7 February 1986   |Psychology        |Mark      |Weber    |12    |14  |07-02-1986 |36  |\n",
      "|May 18, 1987      |Informatics       |Rosie     |Norman   |9     |15  |18-05-1987 |35  |\n",
      "|August 10, 1984   |Business          |Martin    |Steele   |7     |16  |10-08-1984 |37  |\n",
      "|16 December 1990  |Machine Learning  |Colin     |Martinez |9     |17  |16-12-1990 |31  |\n",
      "|unknown           |Data Analytics    |Bridget   |Twain    |6     |18  |unknown    |null|\n",
      "|7 March 1980      |Business          |Darlene   |Mills    |20    |19  |07-03-1980 |42  |\n",
      "|June 2, 1985      |Data Analytics    |Zachary   |--       |10    |20  |02-06-1985 |37  |\n",
      "+------------------+------------------+----------+---------+------+----+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Final dataframe obtained\n",
    "\n",
    "df3.orderBy(\"s_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397869f6",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_yscale.html\n",
    "https://sparkbyexamples.com/pyspark/pyspark-update-a-column-with-value/#:~:text=You%20can%20do%20update%20a,new%20Dataframe%20with%20updated%20values.\n",
    "https://stackoverflow.com/questions/46956026/how-to-convert-column-with-string-type-to-int-form-in-pyspark-data-frame\n",
    "https://stackoverflow.com/questions/63813253/how-to-add-extra-column-with-current-date-in-spark-dataframe\n",
    "https://stackoverflow.com/questions/71889053/pyspark-how-do-i-replace-null-values-based-on-datatype-of-the-column\n",
    "https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/\n",
    "https://stackoverflow.com/questions/58310246/how-to-concatenate-to-a-null-column-in-pyspark-dataframe\n",
    "https://stackoverflow.com/questions/40368877/creating-pyspark-dataframe-column-that-coalesces-two-other-columns-why-am-i-get\n",
    "https://stackoverflow.com/questions/42853778/add-a-column-from-another-dataframe\n",
    "https://sparkbyexamples.com/pyspark/pyspark-to_date-convert-string-to-date-format/\n",
    "https://netflixsub.com/how-to-replace-null-values-with-mean-in-pyspark-dataframe/\n",
    "https://stackoverflow.com/questions/30756939/results-of-reduce-and-count-differ-in-pyspark\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.rightOuterJoin.html\n",
    "https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-to-pyspark-rdd-operations/\n",
    "https://sparkbyexamples.com/pyspark/pyspark-parallelize-create-rdd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49aac8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
